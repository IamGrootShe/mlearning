An introduction to the Hive Metastore Catalog in Databricks, explaining concepts like managed tables, external tables, and views. 
It highlights the differences between managed and external tables, particularly concerning data persistence after dropping a table.

Here are the key takeaways:

Hive Metastore Catalog:

Before Unity Catalog, Hive Metastore was the default catalog in Databricks for storing structured data assets.
It comes with a default schema. If no schema is specified during table creation, the table is created under the default schema.
To use the Hive Metastore, a compute resource (like a serverless warehouse or cluster) is required.

Managed Table:

In a managed table, both metadata (schema) and data are managed by the Metastore.
When a managed table is dropped, both the metadata and the data are permanently deleted.
The default location for data stored in a managed table in Hive Metastore is DBFS (Databricks File System) within the managed resource group.
Managed tables in Databricks are Delta tables by default.

External Table:

Unlike managed tables, for external tables, only the metadata is managed by the Metastore, while the data is stored at a user-defined external location.
When an external table is dropped, only the metadata is removed, and the underlying data remains intact at its specified location. This allows for re-creation of the table using the existing data.

Views:

Views can be created in the Hive Metastore.
They are permanent, meaning they persist even if the cluster is terminated and restarted.
Views are essentially stored queries that retrieve data from underlying tables.

----------------------------------------------------------------------------

The process involves creating a Metastore, setting up an Azure Storage Account for the Metastore data, establishing Access Connectors for Databricks to access the storage, and finally assigning the Metastore to the Databricks Workspace to enable Unity Catalog.

Here are the key steps covered:

Setup Metastore for Databricks :

The Metastore is the top-level container in Unity Catalog.
It's recommended to create only one Metastore per region.
An optional but good practice is to provide an ADLS Gen 2 path for storing managed table data .
Create Azure Storage Account for Metastore :

A storage account is needed to store the Metastore's data.
The video demonstrates creating a storage account in Azure Portal (1:43-3:12), enabling hierarchical namespace for Data Lake Storage Gen2 endpoint .
A root container and a "metastore" directory are created within the storage account to serve as the root for Metastore data .
Access Connectors for Azure Databricks:

An Access Connector for Azure Databricks is created to provide Databricks with identity to connect to the storage account .
This connector is then assigned the "Storage Blob Data Contributor" role on the storage account to grant necessary permissions .
Setup ADLS path for Metastore :

The ADLS path to the storage account container and directory (e.g., root@.dfs.core.windows.net/metastore) is configured in the Metastore creation process .
The Resource ID of the created access connector is also provided .
Assign Metastore to Databricks Workspace :

The newly created Metastore is then assigned to the Databricks Workspace .
This action enables Unity Catalog for the workspace, allowing for catalog creation and management within Databricks .
The user who configured the Metastore automatically becomes the Metastore Admin with privileges to work with catalogs .

-----------------------------------------------

 a detailed guide on creating catalogs with and without external locations in Databricks Unity Catalog, along with steps for creating storage credentials and external locations.

Here are the key takeaways from the video:

Understanding External Locations and Data Storage (0:42)

Metastore Level: By default, managed table data is stored at the metastore location unless a more specific location is defined.
Catalog Level: If an external location is specified at the catalog level, managed tables created under that catalog will store their data in that location.
Schema Level: If a schema-level external location is defined, managed table data will be stored there, overriding catalog and metastore settings.
Fallback Mechanism: If no location is defined at the schema level, it falls back to the catalog level. If no catalog location is defined, it falls back to the metastore level.
Mandatory Catalog Location: If a metastore is created without a specified location, it becomes mandatory to specify a location at the catalog level when creating a catalog.
Creating External Location (2:45)

The video demonstrates creating a new container named "data" and a directory "ADB/catalog" within an existing Azure storage account to serve as the root for catalog data storage.
Creating Catalogs without External Location (3:45)

Catalogs can be created directly from the Databricks UI or using SQL commands.
A catalog named "Dev" is created via the UI without specifying an external location .
SQL is used to create another catalog named "Dev SQL" with a comment .
To view catalog information, use DESCRIBE CATALOG EXTENDED.
Dropping Catalogs 

Attempting to DROP CATALOG directly will fail if the catalog is not empty due to default schemas.
The CASCADE keyword (DROP CATALOG CASCADE) can be used to recursively drop all tables, schemas, and views within the catalog .
Creating Storage Credentials

Storage credentials act as a bridge between Databricks and external storage accounts.
A new storage credential named "UC catalog storage" is created using Azure managed identity and the existing UC connector ID .
Creating External Location in Databricks 

An external location named "EXT catalog" is created in Databricks using SQL, pointing to the Azure storage path and associated with the newly created storage credential .
Creating Catalog with External Location 

A new catalog named "Dev EXT" is created using SQL, explicitly defining the MANAGED LOCATION to the previously created external location .

Describing this catalog (DESCRIBE CATALOG EXTENDED Dev_EXT) shows the storage_root pointing to the specified external location .

--------------------------------------------------
how to create three types of schemas and how managed table data is stored in each:

Schema in a default Dev catalog (0:22): This schema is created in a catalog where no external location is specified at the catalog level.
Schema in a Dev EXT catalog (0:27): This schema is created in a catalog where an external location is specified at the catalog level.
Schema with an external location at the schema level (0:35): This schema is created within the same Dev EXT catalog but has its own external location defined directly at the schema level.
Key Steps and Concepts:

Creating a Folder for External Data (1:00): Before defining an external location for schema-level data, a new folder (e.g., "schema") needs to be created in the ADLS location to store this external data.
Reusing Storage Credentials (1:34): The video emphasizes reusing existing storage credentials from previous setups to connect Databricks to storage accounts.
Creating Schemas (2:15):
Schemas are created using SQL commands like CREATE SCHEMA ..
A schema can be created in a default catalog (2:21) or an external catalog (3:00).
Defining External Locations for Schemas (3:34):
An external location must be defined in Databricks before it can be used for a schema. This is done under the "Catalogs" tab > "External Data" > "External Location" (3:40).
The command CREATE EXTERNAL LOCATION MANAGED LOCATION '' WITH (STORAGE CREDENTIAL ) is used (3:50).
Creating a Schema with an External Location (4:25): The command to create a schema with an external location is CREATE SCHEMA . MANAGED LOCATION '' (4:29).
Validating Managed Table Data Location (5:22):
The video shows how managed tables store data differently based on where the external location is defined.
For schemas in a default catalog without an external location, data is stored under the metastore location (6:46).
For schemas in a catalog with an external location, data is stored at the catalog level (7:48).
For schemas with their own external location, data is stored at the schema level (8:20).
The DESCRIBE EXTENDED .. command is used to view the storage location of the table data (6:48).
Databricks Catalog Explorer (9:14): The Catalog Explorer can be used to view schema and table details, including their IDs and storage locations (9:20).
Unity Catalog stores table data with the table ID as the folder name, not the table name (9:35).
Schema IDs are also used in the storage path for schema-level external locations (10:10).
The video concludes by highlighting that managed table data locations can now be specified at the metastore, catalog, or schema level in Unity Catalog (10:46).

